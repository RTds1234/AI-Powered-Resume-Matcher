{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aa35e60-205c-4eb0-b1b7-9d65affb6b00",
   "metadata": {},
   "source": [
    "## Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7d80749-bc76-4f8d-a76c-4daf92b8f936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94b3cf92-d293-4e64-a88f-a0782523d285",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ronak\\Downloads\\anaconda3\\envs\\resume-matcher\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ronak\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import re\n",
    "import docx\n",
    "import fitz  # PyMuPDF\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load spaCy & sentence-transformers\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Fast, small model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a9b0fc-0e8c-473a-b53b-a31084610feb",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8a7ae7c2-6ac2-4831-ad5c-afa0cd2dceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Text Using spaCy\n",
    "def clean_text(text):\n",
    "    if not text or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [\n",
    "        token.lemma_ for token in doc\n",
    "        if not token.is_stop and not token.is_punct and not token.like_num\n",
    "    ]\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cf5719-9c8f-498c-9014-6254f0585e35",
   "metadata": {},
   "source": [
    "## Resume & JD Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bbb76e1e-aa00-4c8d-a364-d028fd2dce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Text from File (PDF, DOCX, or TXT)\n",
    "def extract_text(file_path):\n",
    "    if file_path.endswith('.pdf'):\n",
    "        text = \"\"\n",
    "        with fitz.open(file_path) as doc:\n",
    "            for page in doc:\n",
    "                text += page.get_text()\n",
    "        return text\n",
    "    elif file_path.endswith('.docx'):\n",
    "        doc = docx.Document(file_path)\n",
    "        return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "    elif file_path.endswith('.txt'):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    else:\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa1405b-6b1c-4e90-94a1-08b6c33211ab",
   "metadata": {},
   "source": [
    "## load and Clean ALL Resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e5e24fa0-0544-485c-b201-27d76effa0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Load and Process All Resumes\n",
    "resume_dir = os.path.abspath(os.path.join(\n",
    "    \"C:\\\\Users\\\\ronak\\\\Downloads\\\\Py_DS_ML_Bootcamp-master\\\\Refactored_Py_DS_ML_Bootcamp-master\\\\AI Resume matcher\", \n",
    "    \"data\", \n",
    "    \"resumes\"\n",
    "))\n",
    "resumes = {}\n",
    "\n",
    "for file in os.listdir(resume_dir):\n",
    "    if file.endswith(('.pdf', '.docx', '.txt')):\n",
    "        path = os.path.join(resume_dir, file)\n",
    "        text = extract_text(path)\n",
    "        cleaned = clean_text(text)\n",
    "        resumes[file] = cleaned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcfa3e9-34af-4913-9aa9-46570b5c1275",
   "metadata": {},
   "source": [
    "## Load and Process Job Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7304778a-5b31-4395-b4b8-98372933e5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "jd_path=os.path.abspath(os.path.join(\n",
    "    \"C:\\\\Users\\\\ronak\\\\Downloads\\\\Py_DS_ML_Bootcamp-master\\\\Refactored_Py_DS_ML_Bootcamp-master\\\\AI Resume matcher\", \n",
    "    \"data\", \n",
    "    \"job_descriptions\",\"Senior Python Developer.txt\"\n",
    "))\n",
    "jd_text=extract_text(jd_path)\n",
    "cleaned_jd=clean_text(jd_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28a9e51-bd4a-4684-b99a-8b08943c066e",
   "metadata": {},
   "source": [
    "## Embed and Score Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eac57a02-bfb8-457d-87c3-aae04bc0add4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ senior-python-developer2 - Template 18.pdf → 78.33% match\n",
      "✅ entry-level-software-engineer2 - Template 17.pdf → 61.14% match\n",
      "✅ Full Stack Web Developer - Template 10.pdf → 53.08% match\n",
      "✅ pl-sql-developer2  - Template 16 .pdf → 52.99% match\n"
     ]
    }
   ],
   "source": [
    "jd_embedding=model.encode(cleaned_jd,convert_to_tensor=True)\n",
    "\n",
    "results=[]\n",
    "for filename,resume_text in resumes.items():\n",
    "    res_embedding=model.encode(resume_text,convert_to_tensor=True)\n",
    "    score=util.cos_sim(jd_embedding,res_embedding).item()\n",
    "    results.append((filename,round(score*100,2))) #percentage\n",
    "#sort by best match\n",
    "results=sorted(results,key=lambda x:x[1],reverse=True)\n",
    "for r in results:\n",
    "    print(f\"✅ {r[0]} → {r[1]}% match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe63ccc-9e18-4408-80a7-6eb0d8246ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (resume-matcher)",
   "language": "python",
   "name": "resume-matcher"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
